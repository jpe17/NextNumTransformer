{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Transformer Learning Journey\n",
      "==================================\n",
      "Understanding transformers from first principles\n",
      "\n",
      "Setting up data loaders...\n",
      "mnist dataset loaded, train data size: 50000 validation data size: 10000\n",
      "✓ Data ready: 391 train batches, 79 val batches\n",
      "✓ Image transformation: [128, 1, 28, 28] → [128, 16, 1, 7, 7]\n",
      "  28x28 images split into 16 patches of 7x7 each\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === VISION TRANSFORMER FROM FIRST PRINCIPLES ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import our clean backend modules\n",
    "from backend import (\n",
    "    setup_data_loaders, debug_data_shapes,\n",
    "    VisionTransformerEncoder, debug_model_architecture, visualize_model_data_flow,\n",
    "    train_model_with_debugging,\n",
    "    analyze_training_history, analyze_predictions, show_prediction_mistakes,\n",
    "    show_learning_insights\n",
    ")\n",
    "\n",
    "print(\"Vision Transformer Learning Journey\")\n",
    "print(\"==================================\")\n",
    "print(\"Understanding transformers from first principles\")\n",
    "print()\n",
    "\n",
    "# Setup data loaders\n",
    "train_patch_loader, val_patch_loader, train_loader, val_loader = setup_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ===\n",
    "\n",
    "print(\"Training the Model\")\n",
    "print(\"==================\")\n",
    "\n",
    "# Train the model with clean insights\n",
    "history = train_model_with_debugging(\n",
    "    model=model,\n",
    "    train_loader=train_patch_loader,\n",
    "    val_loader=val_patch_loader,\n",
    "    epochs=15,\n",
    "    lr=0.0001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ANALYSIS ===\n",
    "\n",
    "print(\"Analyzing Results\")\n",
    "print(\"================\")\n",
    "\n",
    "# Training analysis with visualization\n",
    "analyze_training_history(history)\n",
    "\n",
    "# Prediction analysis\n",
    "prediction_results = analyze_predictions(model, val_patch_loader)\n",
    "\n",
    "# Show mistakes to understand failure cases\n",
    "show_prediction_mistakes(model, val_loader, val_patch_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BUILD VISION TRANSFORMER ===\n",
    "\n",
    "print(\"Building Vision Transformer\")\n",
    "print(\"==========================\")\n",
    "\n",
    "# Create the model\n",
    "model = VisionTransformerEncoder(debug=False)  # We'll debug manually\n",
    "\n",
    "# Get sample data for visualization\n",
    "sample_patches, sample_labels = next(iter(train_patch_loader))\n",
    "sample_images, _ = next(iter(train_loader))\n",
    "\n",
    "print(f\"Sample batch - Patches: {sample_patches.shape}, Labels: {sample_labels.shape}\")\n",
    "print(f\"Model ready! Now let's see how data flows through it...\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DATA FLOW VISUALIZATION ===\n",
    "\n",
    "print(\"Model Data Flow - See How Transformers Process Images\")\n",
    "print(\"====================================================\")\n",
    "\n",
    "# Show complete data transformation through the model\n",
    "model_flow = visualize_model_data_flow(\n",
    "    model=model,\n",
    "    sample_data=sample_patches,\n",
    "    original_images=sample_images,\n",
    "    labels=sample_labels\n",
    ")\n",
    "\n",
    "print(\"This shows the complete journey:\")\n",
    "print(\"1. Image → Patches → Flattened → Embedded\")\n",
    "print(\"2. Add positional info → Transformer layers\")  \n",
    "print(\"3. Self-attention processing → Global pooling\")\n",
    "print(\"4. Final classification → Prediction\")\n",
    "print()\n",
    "\n",
    "# Model architecture overview\n",
    "debug_model_architecture(model, sample_patches[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INSIGHTS ===\n",
    "\n",
    "print(\"Key Insights\")\n",
    "print(\"===========\")\n",
    "\n",
    "# Show learning insights\n",
    "show_learning_insights(history)\n",
    "\n",
    "print(\"What we learned about Vision Transformers:\")\n",
    "print(\"• Patches allow treating images as sequences\")\n",
    "print(\"• Self-attention captures spatial relationships\")\n",
    "print(\"• Multi-head attention captures different features\")\n",
    "print(\"• Positional embeddings encode patch locations\")\n",
    "print(\"• Layer normalization stabilizes training\")\n",
    "print(\"• Residual connections help gradient flow\")\n",
    "\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"• Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"• Best accuracy: {max(history['val_accuracies']):.2f}%\")\n",
    "print(f\"• Epochs trained: {len(history['train_losses'])}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"• Experiment with different hyperparameters\")\n",
    "print(\"• Try on more complex datasets\")\n",
    "print(\"• Visualize attention patterns\")\n",
    "print(\"• Compare with CNN architectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: DEEPER ANALYSIS ===\n",
    "\n",
    "print(\"Optional: Deeper Analysis\")\n",
    "print(\"========================\")\n",
    "print(\"Uncomment sections below for additional insights:\")\n",
    "print()\n",
    "\n",
    "# Additional data analysis\n",
    "debug_data_shapes(train_patch_loader)\n",
    "\n",
    "# Batch statistics\n",
    "from backend.data_processing import analyze_batch_statistics\n",
    "analyze_batch_statistics(train_patch_loader)\n",
    "\n",
    "# Transformer component explanation  \n",
    "from backend.transformer_architecture import explain_transformer_components\n",
    "explain_transformer_components()\n",
    "\n",
    "# Training health analysis\n",
    "from backend.training_engine import analyze_training_health\n",
    "analyze_training_health(history)\n",
    "\n",
    "print(\"These tools help you understand the details behind the scenes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
